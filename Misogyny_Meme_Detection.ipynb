{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djvz6VQZgbQ",
        "outputId": "909854fc-6c7b-439d-df90-f2337a8b2ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "BATCH_SIZE = 8  # Reduced for faster training\n",
        "EPOCHS = 5\n",
        "LR = 1e-4\n"
      ],
      "metadata": {
        "id": "5kvikg2eaacg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Path**"
      ],
      "metadata": {
        "id": "mHS6SWNy-paa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_CSV = '/content/drive/MyDrive/Project 3/train.csv'\n",
        "DEV_CSV = '/content/drive/MyDrive/Project 3/dev.csv'\n",
        "TRAIN_IMG_DIR = '/content/drive/MyDrive/Project 3/train'\n",
        "DEV_IMG_DIR = '/content/drive/MyDrive/Project 3/dev'\n",
        "TEST_IMG_DIR = '/content/drive/MyDrive/Project 3/test'\n"
      ],
      "metadata": {
        "id": "DeUvOBI7avvi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(TRAIN_CSV)\n",
        "print(df.head())\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfX9PaoWf2Xi",
        "outputId": "f3dd7841-872a-4ef6-b7a8-b8cc7dbd3cac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  image_name        labels                  transcriptions\n",
            "0   1134.jpg      Misogyny  如果结婚是好事的话 按照我国的国情 应该托关系走后门才能结婚\n",
            "1    901.jpg  Not-Misogyny                        狠狠期待 好了嘛\n",
            "2    683.jpg      Misogyny         为什么不生孩子？ 让男的生呗 他们不是先生嘛？\n",
            "3    768.jpg  Not-Misogyny        搞不懂呢 你知道滴 我不过是 来自大山深处的吗喽\n",
            "4    178.jpg  Not-Misogyny                 买完同款买同款 家里可以开展览\n",
            "Index(['image_name', 'labels', 'transcriptions'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(DEV_CSV)\n",
        "print(df.head())\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3acSuI3VgijY",
        "outputId": "5490d0d4-080b-48da-aeaf-0507e7ae3631"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  image_name        labels                                     transcriptions\n",
            "0    423.jpg  Not-Misogyny                                   想我的生活...总是差一点就顺呐\n",
            "1    204.jpg      Misogyny                 孩子不听话是你不对 孩子学习成绩不好 还是你不对 EN EN 听我说\n",
            "2    571.jpg      Misogyny                       同事： 你啥时候生孩子啊 我： 你是不是着急投胎叫我妈啊\n",
            "3    323.jpg  Not-Misogyny                                              去你的调休\n",
            "4   1403.jpg  Not-Misogyny  国庆7天+中秋3天 调休完等于总共放4天 上次遇到这种7+3=4的题目 还是树上骑个猴地上三...\n",
            "Index(['image_name', 'labels', 'transcriptions'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**"
      ],
      "metadata": {
        "id": "1LU0Oyxy-xpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, tokenizer, transform, is_test=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "\n",
        "        if not is_test:\n",
        "            self.data = pd.read_csv(csv_file)\n",
        "        else:\n",
        "            self.image_names = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names) if self.is_test else len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            image_name = self.image_names[idx]\n",
        "            image_path = os.path.join(self.image_dir, image_name)\n",
        "            image = self.transform(Image.open(image_path).convert('RGB'))\n",
        "            dummy_text = \"\"\n",
        "            encoded_text = self.tokenizer(dummy_text, padding='max_length', truncation=True, max_length=64, return_tensors='pt')\n",
        "            return image, encoded_text['input_ids'].squeeze(0), encoded_text['attention_mask'].squeeze(0), image_name\n",
        "        else:\n",
        "            image_name = self.data.iloc[idx]['image_name']\n",
        "            image_path = os.path.join(self.image_dir, image_name)\n",
        "            image = self.transform(Image.open(image_path).convert('RGB'))\n",
        "            transcription = self.data.iloc[idx]['transcriptions']\n",
        "            encoded_text = self.tokenizer(transcription, padding='max_length', truncation=True, max_length=64, return_tensors='pt')\n",
        "            label_str = self.data.iloc[idx]['labels']\n",
        "            label = 1 if label_str.lower() == 'misogyny' else 0\n",
        "            return image, encoded_text['input_ids'].squeeze(0), encoded_text['attention_mask'].squeeze(0), label, image_name"
      ],
      "metadata": {
        "id": "uzYtG4dJgnb1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "sample_dataset = MemeDataset(TRAIN_CSV, TRAIN_IMG_DIR, tokenizer, transform)\n",
        "sample_item = sample_dataset[0]\n",
        "print(\"✅ Sample image tensor shape:\", sample_item[0].shape)\n",
        "print(\"✅ Sample text input ids shape:\", sample_item[1].shape)\n",
        "print(\"✅ Sample label:\", sample_item[3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVyt5yVTg4lS",
        "outputId": "49ab50ae-8043-4446-f806-c9c776afcde5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample image tensor shape: torch.Size([3, 224, 224])\n",
            "✅ Sample text input ids shape: torch.Size([64])\n",
            "✅ Sample label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "Wdx9cN0X_lqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.image_model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        self.image_model.fc = nn.Identity()\n",
        "        for param in self.image_model.parameters():  # Freeze ResNet\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        for param in self.text_model.parameters():  # Freeze BERT\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048 + 768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, input_ids, attention_mask):\n",
        "        img_feat = self.image_model(img)\n",
        "        text_feat = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output.detach()\n",
        "        combined = torch.cat((img_feat, text_feat), dim=1)\n",
        "        return self.classifier(combined)"
      ],
      "metadata": {
        "id": "0X2k0jYDg7Bt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalClassifier()\n",
        "img, ids, mask, label, _ = sample_dataset[0]\n",
        "with torch.no_grad():\n",
        "    out = model(img.unsqueeze(0), ids.unsqueeze(0), mask.unsqueeze(0))\n",
        "print(\"✅ Model output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b0ZQUSIhDe7",
        "outputId": "b3a6f8c6-d6be-45f0-f73c-89618e8e0f63"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model output shape: torch.Size([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize**"
      ],
      "metadata": {
        "id": "0_z6jXVg_sA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "model = MultimodalClassifier().to(device)\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=LR)  # Only train classifier\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "2bkt1_MkhH4y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataloader**"
      ],
      "metadata": {
        "id": "wszcM-Jm_hFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MemeDataset(TRAIN_CSV, TRAIN_IMG_DIR, tokenizer, transform)\n",
        "dev_dataset = MemeDataset(DEV_CSV, DEV_IMG_DIR, tokenizer, transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "8xHQrkrChU0F"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch = next(iter(train_loader))\n",
        "print(\"✅ Batch image shape:\", sample_batch[0].shape)\n",
        "print(\"✅ Batch input_ids shape:\", sample_batch[1].shape)\n",
        "print(\"✅ Batch labels:\", sample_batch[3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u2A62Ljhcs1",
        "outputId": "a516139c-8da8-43f0-d651-ed112acf2aa2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch image shape: torch.Size([8, 3, 224, 224])\n",
            "✅ Batch input_ids shape: torch.Size([8, 64])\n",
            "✅ Batch labels: tensor([0, 0, 0, 0, 0, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "--OS0yQy_eeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for img, ids, mask, labels, _ in tqdm(train_loader):\n",
        "        img, ids, mask, labels = img.to(device), ids.to(device), mask.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(img, ids, mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ-K2fwJheWh",
        "outputId": "a2343b62-b088-40ca-d7ff-10b35ce2a0db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [15:55<00:00,  6.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.5570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [09:38<00:00,  3.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Loss: 0.4731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [09:35<00:00,  3.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Loss: 0.4247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [09:36<00:00,  3.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Loss: 0.3853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [09:37<00:00,  3.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Loss: 0.3853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**"
      ],
      "metadata": {
        "id": "e4-dwuZ4_aWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MemeDataset(None, TEST_IMG_DIR, tokenizer, transform, is_test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for img, ids, mask, image_names in tqdm(test_loader):\n",
        "        img, ids, mask = img.to(device), ids.to(device), mask.to(device)\n",
        "        outputs = model(img, ids, mask)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
        "        predictions.extend(zip(image_names, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5FN8NOShoJI",
        "outputId": "f4755402-adca-4862-83c7-ed99558b2476"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 43/43 [02:57<00:00,  4.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame(predictions, columns=['image_name', 'labels'])\n",
        "submission['labels'] = submission['labels'].map({1: 'Misogyny', 0: 'Not-Misogyny'})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ Submission file saved as 'SSNCSE_run1.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o11mnFXrhr8q",
        "outputId": "9d7d94b8-a364-467f-aeff-f1eded15681b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Submission file saved as 'SSNCSE_run1.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification report for Development data**"
      ],
      "metadata": {
        "id": "xeRMMyGf_LwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- Evaluation on Dev Set ---\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for img, ids, mask, labels, _ in tqdm(dev_loader):\n",
        "        img, ids, mask = img.to(device), ids.to(device), mask.to(device)\n",
        "        outputs = model(img, ids, mask)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "# --- Classification Report ---\n",
        "report = classification_report(all_labels, all_preds, target_names=['Not-Misogyny', 'Misogyny'])\n",
        "print(\"📊 Classification Report on Dev Set:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7UgF9TQiLBy",
        "outputId": "2089efd8-5448-412c-ac04-a6b2709aafa1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [02:34<00:00,  7.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Classification Report on Dev Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Not-Misogyny       0.86      0.92      0.89       123\n",
            "    Misogyny       0.74      0.62      0.67        47\n",
            "\n",
            "    accuracy                           0.84       170\n",
            "   macro avg       0.80      0.77      0.78       170\n",
            "weighted avg       0.83      0.84      0.83       170\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}